### Pré-requisitos
- Como o spark-master também é o container do kafka, é necessário criar uma pasta `kafka` dentro da pasta `spark-master` com os arquivos para instalação do kafka e debezium, sendo estes:
  - O Kafka 3.9.0 para Scala 2.12, renomeado para `kafka.tgz` (`wget https://dlcdn.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz -O kafka.tgz` ou obtido navegando pelos downloads no site do kafka https://kafka.apache.org/downloads);
  - O conector do Debezium para Postgres na versão 2.7.4, renomeado para `debezium_postgres.tar.gz` (`wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/2.7.4.Final/debezium-connector-postgres-2.7.4.Final-plugin.tar.gz -O debezium_postgres.tar.gz` ou  navegando no site do debezium https://debezium.io/releases/2.7/).

### Primeira execução (docker compose up), pós criação dos containeres
#### Passos:
- execute `docker compose exec postgres sh -c 'echo wal_level = logical >> /var/lib/postgresql/data/postgresql.conf'` para permitir a conexão do debezium ao postgres;
- reinicie o container do postgres;
- execute `start_kafka.sh` a partir da pasta `spark-master/`;
- execute `submit_streaming_job.sh` partir da pasta `spark-master/`
[setup postgres>debezium>kafka>spark pronto]

### Demais execuções:
- execute `start_kafka.sh` a partir da pasta `spark-master/`;
- execute `submit_streaming_job.sh` partir da pasta `spark-master/`

### Verificações
deixe o `console_consumer.sh` executando e insira dados pelo shell do postgres (`container_psql.sh` começa uma sessão iterativa do psql lá), o consumer deve mostrar todas as mensagens da fila desde o início,
que por sua vez devem ser todos os dados inseridos no kafka.  (testa a interação postgres>debezium>kafka)  
O spark_streaming.py está alterado para enviar a sua saída pelo console, sua execução (simplificada no comando submit_streaming_job.sh) deve refletir os dados da fila do kafka. (testa a integração kafka>spark)